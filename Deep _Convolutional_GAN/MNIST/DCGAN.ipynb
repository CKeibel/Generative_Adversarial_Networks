{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0iOeMBRIvBd"
      },
      "source": [
        "# Deep Convolutional Generative Adversarial Network\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r35tkP1HJBw6"
      },
      "source": [
        "### Loading datset and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68QtTPrkHpGD"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\r\n",
        "\r\n",
        "# load mnist x_train\r\n",
        "(x_train, _), (_, _) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTIwCDfOJerF",
        "outputId": "d32a823e-0346-4b56-d918-441aac0d85d0"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "\"\"\" Preprocessing \"\"\"\r\n",
        "\r\n",
        "# create batch_dim\r\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\r\n",
        "\r\n",
        "# convert to keras compatible float\r\n",
        "x_train = x_train.astype(np.float32)\r\n",
        "\r\n",
        "# normalization to inputs between [-1, 1]\r\n",
        "MAX = np.max(x_train)\r\n",
        "x_train = (x_train / (MAX/2)) - 1\r\n",
        "\r\n",
        "print(f\"Data_shape: {x_train.shape} - Input_range: [{np.min(x_train)}, {np.max(x_train)}]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data_shape: (60000, 28, 28, 1) - Input_range: [-1.0, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8_96kFbLeDz"
      },
      "source": [
        "### Discriminator Network\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0JBRysYLZ6-"
      },
      "source": [
        "from tensorflow.keras.layers import Activation\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import Conv2D\r\n",
        "from tensorflow.keras.layers import Dropout\r\n",
        "from tensorflow.keras.layers import Flatten\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.layers import LeakyReLU\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "\r\n",
        "img_shape = x_train.shape[1:] # (28, 28, 1)\r\n",
        "\r\n",
        "DISCRIMINATOR = Sequential()\r\n",
        "DISCRIMINATOR.add(Conv2D(filters=64, kernel_size=5, strides=2, padding=\"same\", input_shape=img_shape))\r\n",
        "DISCRIMINATOR.add(LeakyReLU(alpha=0.2))\r\n",
        "DISCRIMINATOR.add(Dropout(rate=0.3))\r\n",
        "DISCRIMINATOR.add(Conv2D(filters=128, kernel_size=5, strides=2, padding=\"same\"))\r\n",
        "DISCRIMINATOR.add(LeakyReLU(alpha=0.2))\r\n",
        "DISCRIMINATOR.add(Dropout(rate=0.3))\r\n",
        "DISCRIMINATOR.add(Flatten())\r\n",
        "DISCRIMINATOR.add(Dense(1))\r\n",
        "DISCRIMINATOR.add(Activation(\"sigmoid\")) # sigmoid range [0,1] 0 - fake IMG, 1 real IMG\r\n",
        "\r\n",
        "d_input = Input(shape=img_shape) # image in\r\n",
        "d_pred = DISCRIMINATOR(d_input) # prediction in % out\r\n",
        "\r\n",
        "DISCRIMINATOR = Model(inputs=d_input, outputs=d_pred)\r\n",
        "DISCRIMINATOR.compile(\r\n",
        "    loss=\"binary_crossentropy\",\r\n",
        "    optimizer=Adam(learning_rate=0.0002, beta_1=0.5),\r\n",
        "    metrics=[\"accuracy\"]\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BZPJ67uPD8C"
      },
      "source": [
        "### Generator Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eP1eEG1PF-u"
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.layers import Reshape\r\n",
        "from tensorflow.keras.layers import UpSampling2D\r\n",
        "\r\n",
        "z_dim = 100 # Noise Vector size\r\n",
        "\r\n",
        "GENERATOR = Sequential()\r\n",
        "GENERATOR.add(Dense(units=7 * 7 * 128, input_dim=z_dim)) # 7x7 IMG -> upsacling twice to 28x28 (128 is kernel_dim)\r\n",
        "GENERATOR.add(LeakyReLU(alpha=0.2))\r\n",
        "GENERATOR.add(Reshape(target_shape=(7, 7, 128)))\r\n",
        "GENERATOR.add(UpSampling2D()) # 14x14x128\r\n",
        "GENERATOR.add(Conv2D(filters=128, kernel_size=5, strides=1, padding=\"same\", use_bias=False))\r\n",
        "GENERATOR.add(BatchNormalization())\r\n",
        "GENERATOR.add(LeakyReLU(alpha=0.2))\r\n",
        "GENERATOR.add(UpSampling2D())\r\n",
        "GENERATOR.add(Conv2D(filters=64, kernel_size=5, strides=1, padding=\"same\", use_bias=False))\r\n",
        "GENERATOR.add(BatchNormalization())\r\n",
        "GENERATOR.add(LeakyReLU(alpha=0.2))\r\n",
        "GENERATOR.add(Conv2D(filters=img_shape[-1], kernel_size=5, strides=1, padding=\"same\", use_bias=False))\r\n",
        "GENERATOR.add(Activation(\"tanh\"))\r\n",
        "\r\n",
        "noise = Input(shape=(z_dim,)) # noise in\r\n",
        "img = GENERATOR(noise) # generated image out\r\n",
        "\r\n",
        "GENERATOR = Model(inputs=noise, outputs=img)\r\n",
        "# GENERATOR isn't compiled, because of GAN Net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1baHJqC7RjW0"
      },
      "source": [
        "### DCGAN Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x4YOCfcRir1"
      },
      "source": [
        "# combining DISCRIMINATOR and GENERATOR to GAN\r\n",
        "noise_in = Input(shape=(z_dim,)) # noise in\r\n",
        "img = GENERATOR(noise_in) # generated image\r\n",
        "DISCRIMINATOR.trainable = False\r\n",
        "d_pred = DISCRIMINATOR(img) # prediction if image input is real or fake\r\n",
        "DCGAN = Model(inputs=noise_in, outputs=d_pred)\r\n",
        "\r\n",
        "DCGAN.compile(\r\n",
        "    loss=\"binary_crossentropy\",\r\n",
        "    optimizer=Adam(learning_rate=0.0001)\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ID_tCEFUyJ-"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSxyq2ACUWKE",
        "outputId": "dabf2048-abc1-4d56-dd1f-ece69778547b"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import os\r\n",
        "\r\n",
        "PATH = \"generated_images\"\r\n",
        "try:\r\n",
        "    os.makedirs(PATH)\r\n",
        "except FileExistsError:\r\n",
        "    pass\r\n",
        "\r\n",
        "# creating labels for dataset\r\n",
        "batch_size = 32\r\n",
        "y_real = np.ones(shape=(batch_size, 1)) # 1 = 100% real\r\n",
        "y_fake = np.zeros(shape=(batch_size, 1)) # 0 = 0% real\r\n",
        "\r\n",
        "epochs=20_000\r\n",
        "\r\n",
        "for epoch in range(1, epochs+1):\r\n",
        "    # get random images from real dataset\r\n",
        "    rand_idxs = np.random.randint(low=0, high=x_train.shape[0], size=batch_size)\r\n",
        "    real_imgs = x_train[rand_idxs]\r\n",
        "    # generate fake images for training\r\n",
        "    noise = np.random.normal(loc=0.0, scale=1.0, size=(batch_size, z_dim)) # noise vector (32, 100)\r\n",
        "    fake_imgs = GENERATOR(noise, training=False)\r\n",
        "    # train DISCRIMINATOR\r\n",
        "    d_loss_real = DISCRIMINATOR.train_on_batch(real_imgs, y_real)\r\n",
        "    d_loss_fake = DISCRIMINATOR.train_on_batch(fake_imgs, y_fake)\r\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\r\n",
        "    # train GENERATOR\r\n",
        "    noise = np.random.normal(loc=0.0, scale=1.0, size=(batch_size, z_dim))\r\n",
        "    g_loss = DCGAN.train_on_batch(noise, y_real)\r\n",
        "\r\n",
        "    if(epoch % 100)==0:\r\n",
        "        print(\r\n",
        "            f\"{epoch} - D_loss: {round(d_loss[0], 4)}\"\r\n",
        "            f\" D_acc: {round(d_loss[1], 4)}\"\r\n",
        "            f\" G_loss: {round(g_loss, 4)}\"\r\n",
        "        )\r\n",
        "\r\n",
        "    # saving every 1000 steps\r\n",
        "    if(epoch % 1000) == 0:\r\n",
        "        print(\"SAVED\")\r\n",
        "\r\n",
        "        rows, cols = 5, 5\r\n",
        "        gen_imgs = GENERATOR.predict(noise) # generating image\r\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\r\n",
        "        fig, axs = plt.subplots(rows, cols)\r\n",
        "        cnt = 0\r\n",
        "        for i in range(rows):\r\n",
        "            for j in range(cols):\r\n",
        "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap=\"gray\")\r\n",
        "                axs[i, j].axis(\"off\")\r\n",
        "                cnt +=1\r\n",
        "        img_name = f\"{epoch}.png\"\r\n",
        "        fig.savefig(os.path.join(PATH, img_name))\r\n",
        "        plt.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 - D_loss: 0.5201 D_acc: 0.8125 G_loss: 1.0727\n",
            "200 - D_loss: 0.592 D_acc: 0.7188 G_loss: 0.9452\n",
            "300 - D_loss: 0.684 D_acc: 0.5469 G_loss: 1.0026\n",
            "400 - D_loss: 0.6206 D_acc: 0.6562 G_loss: 0.8844\n",
            "500 - D_loss: 0.6684 D_acc: 0.5156 G_loss: 0.8493\n",
            "600 - D_loss: 0.5861 D_acc: 0.6562 G_loss: 1.0913\n",
            "700 - D_loss: 0.6401 D_acc: 0.6875 G_loss: 0.8195\n",
            "800 - D_loss: 0.6406 D_acc: 0.625 G_loss: 0.8371\n",
            "900 - D_loss: 0.6162 D_acc: 0.6406 G_loss: 0.8277\n",
            "1000 - D_loss: 0.6849 D_acc: 0.5938 G_loss: 0.8145\n",
            "SAVED\n",
            "1100 - D_loss: 0.6426 D_acc: 0.6094 G_loss: 0.9236\n",
            "1200 - D_loss: 0.5965 D_acc: 0.7188 G_loss: 0.8803\n",
            "1300 - D_loss: 0.6222 D_acc: 0.6562 G_loss: 1.0084\n",
            "1400 - D_loss: 0.7089 D_acc: 0.5469 G_loss: 0.7986\n",
            "1500 - D_loss: 0.689 D_acc: 0.5312 G_loss: 0.8259\n",
            "1600 - D_loss: 0.6613 D_acc: 0.6094 G_loss: 0.7718\n",
            "1700 - D_loss: 0.6504 D_acc: 0.6406 G_loss: 0.848\n",
            "1800 - D_loss: 0.6218 D_acc: 0.6719 G_loss: 0.8394\n",
            "1900 - D_loss: 0.5926 D_acc: 0.7188 G_loss: 0.8434\n",
            "2000 - D_loss: 0.6883 D_acc: 0.5469 G_loss: 0.8205\n",
            "SAVED\n",
            "2100 - D_loss: 0.6719 D_acc: 0.6406 G_loss: 0.8999\n",
            "2200 - D_loss: 0.6977 D_acc: 0.5469 G_loss: 0.8588\n",
            "2300 - D_loss: 0.6392 D_acc: 0.6094 G_loss: 0.8278\n",
            "2400 - D_loss: 0.62 D_acc: 0.6875 G_loss: 0.7989\n",
            "2500 - D_loss: 0.6535 D_acc: 0.5 G_loss: 0.8343\n",
            "2600 - D_loss: 0.6159 D_acc: 0.6875 G_loss: 0.8555\n",
            "2700 - D_loss: 0.5667 D_acc: 0.7656 G_loss: 1.0719\n",
            "2800 - D_loss: 0.6417 D_acc: 0.625 G_loss: 0.9221\n",
            "2900 - D_loss: 0.6473 D_acc: 0.6875 G_loss: 0.7746\n",
            "3000 - D_loss: 0.6425 D_acc: 0.6094 G_loss: 0.9406\n",
            "SAVED\n",
            "3100 - D_loss: 0.6565 D_acc: 0.5625 G_loss: 0.8474\n",
            "3200 - D_loss: 0.6117 D_acc: 0.6719 G_loss: 0.8427\n",
            "3300 - D_loss: 0.6423 D_acc: 0.5938 G_loss: 0.8609\n",
            "3400 - D_loss: 0.6683 D_acc: 0.5312 G_loss: 0.8383\n",
            "3500 - D_loss: 0.669 D_acc: 0.5781 G_loss: 0.8335\n",
            "3600 - D_loss: 0.674 D_acc: 0.5625 G_loss: 0.7331\n",
            "3700 - D_loss: 0.7157 D_acc: 0.4844 G_loss: 0.8659\n",
            "3800 - D_loss: 0.6897 D_acc: 0.5312 G_loss: 0.7577\n",
            "3900 - D_loss: 0.6416 D_acc: 0.625 G_loss: 0.8465\n",
            "4000 - D_loss: 0.6078 D_acc: 0.7344 G_loss: 0.9534\n",
            "SAVED\n",
            "4100 - D_loss: 0.6931 D_acc: 0.5625 G_loss: 0.8993\n",
            "4200 - D_loss: 0.6905 D_acc: 0.5625 G_loss: 0.8732\n",
            "4300 - D_loss: 0.6777 D_acc: 0.5312 G_loss: 0.8877\n",
            "4400 - D_loss: 0.6013 D_acc: 0.7188 G_loss: 0.853\n",
            "4500 - D_loss: 0.691 D_acc: 0.5938 G_loss: 0.8606\n",
            "4600 - D_loss: 0.6417 D_acc: 0.6094 G_loss: 0.8327\n",
            "4700 - D_loss: 0.6308 D_acc: 0.625 G_loss: 0.7928\n",
            "4800 - D_loss: 0.7122 D_acc: 0.4531 G_loss: 0.7456\n",
            "4900 - D_loss: 0.6311 D_acc: 0.6406 G_loss: 0.8088\n",
            "5000 - D_loss: 0.7025 D_acc: 0.5938 G_loss: 0.8397\n",
            "SAVED\n",
            "5100 - D_loss: 0.6574 D_acc: 0.6562 G_loss: 0.7606\n",
            "5200 - D_loss: 0.6605 D_acc: 0.625 G_loss: 0.8886\n",
            "5300 - D_loss: 0.6773 D_acc: 0.6562 G_loss: 0.9473\n",
            "5400 - D_loss: 0.6596 D_acc: 0.5938 G_loss: 0.8666\n",
            "5500 - D_loss: 0.6746 D_acc: 0.5781 G_loss: 0.9132\n",
            "5600 - D_loss: 0.6271 D_acc: 0.7031 G_loss: 0.9432\n",
            "5700 - D_loss: 0.6055 D_acc: 0.6562 G_loss: 0.8915\n",
            "5800 - D_loss: 0.5982 D_acc: 0.7344 G_loss: 0.8436\n",
            "5900 - D_loss: 0.7234 D_acc: 0.4844 G_loss: 0.9537\n",
            "6000 - D_loss: 0.6487 D_acc: 0.6719 G_loss: 0.8307\n",
            "SAVED\n",
            "6100 - D_loss: 0.6275 D_acc: 0.6406 G_loss: 0.8863\n",
            "6200 - D_loss: 0.6567 D_acc: 0.5781 G_loss: 0.9365\n",
            "6300 - D_loss: 0.6355 D_acc: 0.6562 G_loss: 0.8763\n",
            "6400 - D_loss: 0.6877 D_acc: 0.5312 G_loss: 0.9255\n",
            "6500 - D_loss: 0.6707 D_acc: 0.6094 G_loss: 0.8427\n",
            "6600 - D_loss: 0.6991 D_acc: 0.4844 G_loss: 0.9057\n",
            "6700 - D_loss: 0.626 D_acc: 0.5938 G_loss: 0.8401\n",
            "6800 - D_loss: 0.6716 D_acc: 0.5469 G_loss: 0.8327\n",
            "6900 - D_loss: 0.6395 D_acc: 0.6406 G_loss: 0.8669\n",
            "7000 - D_loss: 0.6104 D_acc: 0.7188 G_loss: 0.8357\n",
            "SAVED\n",
            "7100 - D_loss: 0.6654 D_acc: 0.6094 G_loss: 0.8274\n",
            "7200 - D_loss: 0.6386 D_acc: 0.6406 G_loss: 0.8883\n",
            "7300 - D_loss: 0.6447 D_acc: 0.5781 G_loss: 0.8117\n",
            "7400 - D_loss: 0.6596 D_acc: 0.5938 G_loss: 0.8171\n",
            "7500 - D_loss: 0.6291 D_acc: 0.6094 G_loss: 0.8204\n",
            "7600 - D_loss: 0.6882 D_acc: 0.5625 G_loss: 0.9188\n",
            "7700 - D_loss: 0.5671 D_acc: 0.7344 G_loss: 0.8373\n",
            "7800 - D_loss: 0.6597 D_acc: 0.5312 G_loss: 0.805\n",
            "7900 - D_loss: 0.6678 D_acc: 0.6562 G_loss: 0.9286\n",
            "8000 - D_loss: 0.6531 D_acc: 0.7031 G_loss: 0.7801\n",
            "SAVED\n",
            "8100 - D_loss: 0.64 D_acc: 0.6719 G_loss: 0.7966\n",
            "8200 - D_loss: 0.6673 D_acc: 0.5938 G_loss: 0.8603\n",
            "8300 - D_loss: 0.6981 D_acc: 0.5312 G_loss: 0.8279\n",
            "8400 - D_loss: 0.6165 D_acc: 0.7188 G_loss: 0.9147\n",
            "8500 - D_loss: 0.6303 D_acc: 0.6094 G_loss: 0.7975\n",
            "8600 - D_loss: 0.6515 D_acc: 0.5781 G_loss: 0.859\n",
            "8700 - D_loss: 0.6507 D_acc: 0.6719 G_loss: 0.7863\n",
            "8800 - D_loss: 0.708 D_acc: 0.5625 G_loss: 0.6799\n",
            "8900 - D_loss: 0.6527 D_acc: 0.6094 G_loss: 0.7776\n",
            "9000 - D_loss: 0.6888 D_acc: 0.5938 G_loss: 0.8272\n",
            "SAVED\n",
            "9100 - D_loss: 0.7081 D_acc: 0.5 G_loss: 0.7964\n",
            "9200 - D_loss: 0.6624 D_acc: 0.5625 G_loss: 0.7645\n",
            "9300 - D_loss: 0.7216 D_acc: 0.5 G_loss: 0.8128\n",
            "9400 - D_loss: 0.5869 D_acc: 0.7188 G_loss: 0.9055\n",
            "9500 - D_loss: 0.6445 D_acc: 0.6406 G_loss: 0.8714\n",
            "9600 - D_loss: 0.652 D_acc: 0.6719 G_loss: 0.8646\n",
            "9700 - D_loss: 0.6611 D_acc: 0.625 G_loss: 0.8943\n",
            "9800 - D_loss: 0.6828 D_acc: 0.5469 G_loss: 0.7748\n",
            "9900 - D_loss: 0.6668 D_acc: 0.625 G_loss: 0.8852\n",
            "10000 - D_loss: 0.657 D_acc: 0.625 G_loss: 0.7201\n",
            "SAVED\n",
            "10100 - D_loss: 0.6783 D_acc: 0.5625 G_loss: 0.8941\n",
            "10200 - D_loss: 0.6651 D_acc: 0.6094 G_loss: 0.8684\n",
            "10300 - D_loss: 0.6788 D_acc: 0.625 G_loss: 0.7935\n",
            "10400 - D_loss: 0.6508 D_acc: 0.6406 G_loss: 0.7921\n",
            "10500 - D_loss: 0.6685 D_acc: 0.625 G_loss: 0.7829\n",
            "10600 - D_loss: 0.6518 D_acc: 0.6406 G_loss: 0.8197\n",
            "10700 - D_loss: 0.7632 D_acc: 0.4688 G_loss: 0.8254\n",
            "10800 - D_loss: 0.6718 D_acc: 0.5469 G_loss: 0.8594\n",
            "10900 - D_loss: 0.6866 D_acc: 0.5625 G_loss: 0.8381\n",
            "11000 - D_loss: 0.6551 D_acc: 0.5781 G_loss: 0.8365\n",
            "SAVED\n",
            "11100 - D_loss: 0.6301 D_acc: 0.6406 G_loss: 0.8444\n",
            "11200 - D_loss: 0.6656 D_acc: 0.5781 G_loss: 0.8355\n",
            "11300 - D_loss: 0.6663 D_acc: 0.5938 G_loss: 0.7601\n",
            "11400 - D_loss: 0.6607 D_acc: 0.5625 G_loss: 0.8484\n",
            "11500 - D_loss: 0.6912 D_acc: 0.5312 G_loss: 0.8621\n",
            "11600 - D_loss: 0.64 D_acc: 0.6719 G_loss: 0.925\n",
            "11700 - D_loss: 0.6069 D_acc: 0.7344 G_loss: 0.8647\n",
            "11800 - D_loss: 0.6849 D_acc: 0.5625 G_loss: 0.8493\n",
            "11900 - D_loss: 0.6069 D_acc: 0.7188 G_loss: 0.862\n",
            "12000 - D_loss: 0.7049 D_acc: 0.5312 G_loss: 0.7947\n",
            "SAVED\n",
            "12100 - D_loss: 0.6597 D_acc: 0.5469 G_loss: 0.7808\n",
            "12200 - D_loss: 0.7355 D_acc: 0.4844 G_loss: 0.821\n",
            "12300 - D_loss: 0.6538 D_acc: 0.5938 G_loss: 0.7943\n",
            "12400 - D_loss: 0.6554 D_acc: 0.6094 G_loss: 0.9338\n",
            "12500 - D_loss: 0.6566 D_acc: 0.5938 G_loss: 0.8399\n",
            "12600 - D_loss: 0.6543 D_acc: 0.7031 G_loss: 0.8322\n",
            "12700 - D_loss: 0.6589 D_acc: 0.5625 G_loss: 0.8179\n",
            "12800 - D_loss: 0.6621 D_acc: 0.625 G_loss: 0.722\n",
            "12900 - D_loss: 0.6562 D_acc: 0.625 G_loss: 0.7899\n",
            "13000 - D_loss: 0.6259 D_acc: 0.6562 G_loss: 0.7762\n",
            "SAVED\n",
            "13100 - D_loss: 0.6878 D_acc: 0.5938 G_loss: 0.793\n",
            "13200 - D_loss: 0.6552 D_acc: 0.6094 G_loss: 0.8656\n",
            "13300 - D_loss: 0.6481 D_acc: 0.6719 G_loss: 0.7919\n",
            "13400 - D_loss: 0.6812 D_acc: 0.5156 G_loss: 0.8684\n",
            "13500 - D_loss: 0.6826 D_acc: 0.5938 G_loss: 0.7411\n",
            "13600 - D_loss: 0.6483 D_acc: 0.6406 G_loss: 0.8234\n",
            "13700 - D_loss: 0.6264 D_acc: 0.6406 G_loss: 0.8413\n",
            "13800 - D_loss: 0.6676 D_acc: 0.5625 G_loss: 0.8342\n",
            "13900 - D_loss: 0.6848 D_acc: 0.5938 G_loss: 0.7448\n",
            "14000 - D_loss: 0.6358 D_acc: 0.6719 G_loss: 0.8684\n",
            "SAVED\n",
            "14100 - D_loss: 0.6662 D_acc: 0.5781 G_loss: 0.7782\n",
            "14200 - D_loss: 0.6643 D_acc: 0.5938 G_loss: 0.8455\n",
            "14300 - D_loss: 0.6197 D_acc: 0.6562 G_loss: 0.7791\n",
            "14400 - D_loss: 0.6707 D_acc: 0.5625 G_loss: 0.8862\n",
            "14500 - D_loss: 0.6728 D_acc: 0.5781 G_loss: 0.8168\n",
            "14600 - D_loss: 0.615 D_acc: 0.6562 G_loss: 0.8384\n",
            "14700 - D_loss: 0.6512 D_acc: 0.5781 G_loss: 0.7797\n",
            "14800 - D_loss: 0.6456 D_acc: 0.5781 G_loss: 0.7618\n",
            "14900 - D_loss: 0.6836 D_acc: 0.5469 G_loss: 0.9207\n",
            "15000 - D_loss: 0.6739 D_acc: 0.6094 G_loss: 0.7714\n",
            "SAVED\n",
            "15100 - D_loss: 0.7008 D_acc: 0.5625 G_loss: 0.8516\n",
            "15200 - D_loss: 0.6843 D_acc: 0.5781 G_loss: 0.7888\n",
            "15300 - D_loss: 0.6391 D_acc: 0.6562 G_loss: 0.7881\n",
            "15400 - D_loss: 0.6762 D_acc: 0.5781 G_loss: 0.9466\n",
            "15500 - D_loss: 0.7195 D_acc: 0.4844 G_loss: 0.7788\n",
            "15600 - D_loss: 0.6318 D_acc: 0.5312 G_loss: 0.7648\n",
            "15700 - D_loss: 0.672 D_acc: 0.6406 G_loss: 0.819\n",
            "15800 - D_loss: 0.5969 D_acc: 0.75 G_loss: 0.9152\n",
            "15900 - D_loss: 0.6189 D_acc: 0.6562 G_loss: 0.702\n",
            "16000 - D_loss: 0.6706 D_acc: 0.625 G_loss: 0.8481\n",
            "SAVED\n",
            "16100 - D_loss: 0.6508 D_acc: 0.5625 G_loss: 0.8064\n",
            "16200 - D_loss: 0.7014 D_acc: 0.6094 G_loss: 0.7369\n",
            "16300 - D_loss: 0.6855 D_acc: 0.6094 G_loss: 0.8216\n",
            "16400 - D_loss: 0.6467 D_acc: 0.5625 G_loss: 0.8414\n",
            "16500 - D_loss: 0.6891 D_acc: 0.5625 G_loss: 0.7777\n",
            "16600 - D_loss: 0.7004 D_acc: 0.5781 G_loss: 0.8508\n",
            "16700 - D_loss: 0.6991 D_acc: 0.5 G_loss: 0.733\n",
            "16800 - D_loss: 0.666 D_acc: 0.6094 G_loss: 0.7875\n",
            "16900 - D_loss: 0.694 D_acc: 0.5 G_loss: 0.8102\n",
            "17000 - D_loss: 0.6468 D_acc: 0.6094 G_loss: 0.7912\n",
            "SAVED\n",
            "17100 - D_loss: 0.6813 D_acc: 0.5781 G_loss: 0.8033\n",
            "17200 - D_loss: 0.7234 D_acc: 0.5312 G_loss: 0.9032\n",
            "17300 - D_loss: 0.6824 D_acc: 0.5 G_loss: 0.7683\n",
            "17400 - D_loss: 0.6617 D_acc: 0.5938 G_loss: 0.732\n",
            "17500 - D_loss: 0.7041 D_acc: 0.4531 G_loss: 0.8644\n",
            "17600 - D_loss: 0.6238 D_acc: 0.6094 G_loss: 0.7991\n",
            "17700 - D_loss: 0.6279 D_acc: 0.6094 G_loss: 0.8104\n",
            "17800 - D_loss: 0.6483 D_acc: 0.625 G_loss: 0.8044\n",
            "17900 - D_loss: 0.6381 D_acc: 0.625 G_loss: 0.8281\n",
            "18000 - D_loss: 0.7064 D_acc: 0.5469 G_loss: 0.8554\n",
            "SAVED\n",
            "18100 - D_loss: 0.6617 D_acc: 0.6406 G_loss: 0.7703\n",
            "18200 - D_loss: 0.7352 D_acc: 0.4375 G_loss: 0.7778\n",
            "18300 - D_loss: 0.6466 D_acc: 0.5625 G_loss: 0.8533\n",
            "18400 - D_loss: 0.6624 D_acc: 0.6562 G_loss: 0.7524\n",
            "18500 - D_loss: 0.6055 D_acc: 0.7031 G_loss: 0.7183\n",
            "18600 - D_loss: 0.6631 D_acc: 0.5312 G_loss: 0.8515\n",
            "18700 - D_loss: 0.6718 D_acc: 0.5156 G_loss: 0.8577\n",
            "18800 - D_loss: 0.6829 D_acc: 0.5781 G_loss: 0.731\n",
            "18900 - D_loss: 0.6639 D_acc: 0.625 G_loss: 0.7701\n",
            "19000 - D_loss: 0.7001 D_acc: 0.4844 G_loss: 0.8116\n",
            "SAVED\n",
            "19100 - D_loss: 0.6693 D_acc: 0.5938 G_loss: 0.8952\n",
            "19200 - D_loss: 0.6627 D_acc: 0.625 G_loss: 0.856\n",
            "19300 - D_loss: 0.6169 D_acc: 0.6875 G_loss: 0.8608\n",
            "19400 - D_loss: 0.6727 D_acc: 0.5625 G_loss: 0.8613\n",
            "19500 - D_loss: 0.73 D_acc: 0.4844 G_loss: 0.7708\n",
            "19600 - D_loss: 0.6951 D_acc: 0.5469 G_loss: 0.6894\n",
            "19700 - D_loss: 0.6471 D_acc: 0.5938 G_loss: 0.7497\n",
            "19800 - D_loss: 0.6465 D_acc: 0.7031 G_loss: 0.8134\n",
            "19900 - D_loss: 0.6975 D_acc: 0.5156 G_loss: 0.8753\n",
            "20000 - D_loss: 0.6416 D_acc: 0.6875 G_loss: 0.7771\n",
            "SAVED\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}