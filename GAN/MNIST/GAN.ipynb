{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0iOeMBRIvBd"
      },
      "source": [
        "# Generative Adversarial Network\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r35tkP1HJBw6"
      },
      "source": [
        "### Loading datset and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68QtTPrkHpGD"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\r\n",
        "\r\n",
        "# load mnist x_train\r\n",
        "(x_train, _), (_, _) = mnist.load_data()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTIwCDfOJerF",
        "outputId": "fb684336-8344-4c92-c657-0491e453071c"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "\"\"\" Preprocessing \"\"\"\r\n",
        "\r\n",
        "# create batch_dim\r\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\r\n",
        "\r\n",
        "# convert to keras compatible float\r\n",
        "x_train = x_train.astype(np.float32)\r\n",
        "\r\n",
        "# normalization to inputs between [-1, 1]\r\n",
        "MAX = np.max(x_train)\r\n",
        "x_train = (x_train / (MAX/2)) - 1\r\n",
        "\r\n",
        "print(f\"Data_shape: {x_train.shape} - Input_range: [{np.min(x_train)}, {np.max(x_train)}]\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data_shape: (60000, 28, 28, 1) - Input_range: [-1.0, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8_96kFbLeDz"
      },
      "source": [
        "### Discriminator Network\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0JBRysYLZ6-"
      },
      "source": [
        "from tensorflow.keras.layers import Activation\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import Flatten\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.layers import LeakyReLU\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "\r\n",
        "img_shape = x_train.shape[1:] # (28, 28, 1)\r\n",
        "\r\n",
        "DISCRIMINATOR = Sequential()\r\n",
        "DISCRIMINATOR.add(Flatten(input_shape=img_shape))\r\n",
        "DISCRIMINATOR.add(Dense(512))\r\n",
        "DISCRIMINATOR.add(LeakyReLU(alpha=0.2)) # alpha -> negative input * alpha\r\n",
        "DISCRIMINATOR.add(Dense(256))\r\n",
        "DISCRIMINATOR.add(LeakyReLU(alpha=0.2))\r\n",
        "DISCRIMINATOR.add(Dense(1))\r\n",
        "DISCRIMINATOR.add(Activation(\"sigmoid\")) # sigmoid range [0,1] 0 - fake IMG, 1 real IMG\r\n",
        "\r\n",
        "d_input = Input(shape=img_shape) # image in\r\n",
        "d_pred = DISCRIMINATOR(d_input) # prediction in % out\r\n",
        "\r\n",
        "DISCRIMINATOR = Model(inputs=d_input, outputs=d_pred)\r\n",
        "DISCRIMINATOR.compile(\r\n",
        "    loss=\"binary_crossentropy\",\r\n",
        "    optimizer=Adam(learning_rate=0.0002, beta_1=0.5),\r\n",
        "    metrics=[\"accuracy\"]\r\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BZPJ67uPD8C"
      },
      "source": [
        "### Generator Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eP1eEG1PF-u"
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.layers import Reshape\r\n",
        "\r\n",
        "z_dim = 100 # Noise Vector size\r\n",
        "\r\n",
        "GENERATOR = Sequential()\r\n",
        "GENERATOR.add(Dense(units=256, input_dim=z_dim))\r\n",
        "GENERATOR.add(LeakyReLU(alpha=0.2))\r\n",
        "GENERATOR.add(BatchNormalization(momentum=0.8))\r\n",
        "GENERATOR.add(Dense(512))\r\n",
        "GENERATOR.add(LeakyReLU(alpha=0.2))\r\n",
        "GENERATOR.add(BatchNormalization(momentum=0.8))\r\n",
        "GENERATOR.add(Dense(1024))\r\n",
        "GENERATOR.add(LeakyReLU(alpha=0.2))\r\n",
        "GENERATOR.add(BatchNormalization(momentum=0.8))\r\n",
        "GENERATOR.add(Dense(np.prod(img_shape)))\r\n",
        "GENERATOR.add(Activation(\"tanh\"))\r\n",
        "GENERATOR.add(Reshape(target_shape=img_shape))\r\n",
        "\r\n",
        "noise = Input(shape=(z_dim,)) # noise in\r\n",
        "img = GENERATOR(noise) # generated image out\r\n",
        "\r\n",
        "GENERATOR = Model(inputs=noise, outputs=img)\r\n",
        "# GENERATOR isn't compiled, because of GAN Net"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1baHJqC7RjW0"
      },
      "source": [
        "### GAN Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x4YOCfcRir1"
      },
      "source": [
        "# combining DISCRIMINATOR and GENERATOR to GAN\r\n",
        "noise_in = Input(shape=(z_dim,)) # noise in\r\n",
        "img = GENERATOR(noise_in) # generated image\r\n",
        "DISCRIMINATOR.trainable = False\r\n",
        "d_pred = DISCRIMINATOR(img) # prediction if image input is real or fake\r\n",
        "GAN = Model(inputs=noise_in, outputs=d_pred)\r\n",
        "\r\n",
        "GAN.compile(\r\n",
        "    loss=\"binary_crossentropy\",\r\n",
        "    optimizer=Adam(learning_rate=0.0002, beta_1=0.5)\r\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ID_tCEFUyJ-"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSxyq2ACUWKE",
        "outputId": "6928114f-1f6c-4edb-c114-4bb71f1259e4"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import os\r\n",
        "\r\n",
        "PATH = \"generated_images\"\r\n",
        "try:\r\n",
        "    os.makedirs(PATH)\r\n",
        "except FileExistsError:\r\n",
        "    pass\r\n",
        "\r\n",
        "# creating labels for dataset\r\n",
        "batch_size = 32\r\n",
        "y_real = np.ones(shape=(batch_size, 1)) # 1 = 100% real\r\n",
        "y_fake = np.zeros(shape=(batch_size, 1)) # 0 = 0% real\r\n",
        "\r\n",
        "epochs=10_000\r\n",
        "\r\n",
        "for epoch in range(epochs):\r\n",
        "    # get random images from real dataset\r\n",
        "    rand_idxs = np.random.randint(low=0, high=x_train.shape[0], size=batch_size)\r\n",
        "    real_imgs = x_train[rand_idxs]\r\n",
        "    # generate fake images for training\r\n",
        "    noise = np.random.normal(loc=0.0, scale=1.0, size=(batch_size, z_dim)) # noise vector (32, 100)\r\n",
        "    fake_imgs = GENERATOR(noise, training=False)\r\n",
        "    # train DISCRIMINATOR\r\n",
        "    d_loss_real = DISCRIMINATOR.train_on_batch(real_imgs, y_real)\r\n",
        "    d_loss_fake = DISCRIMINATOR.train_on_batch(fake_imgs, y_fake)\r\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\r\n",
        "    # train GENERATOR\r\n",
        "    noise = np.random.normal(loc=0.0, scale=1.0, size=(batch_size, z_dim))\r\n",
        "    g_loss = GAN.train_on_batch(noise, y_real)\r\n",
        "\r\n",
        "    # saving every 1000 steps\r\n",
        "    if(epoch+1 % 1000) == 0:\r\n",
        "        print(\r\n",
        "            f\"{epoch} - D_loss: {round(d_loss[0], 4)}\"\r\n",
        "            f\" D_acc: {round(d_loss[1], 4)}\"\r\n",
        "            f\" G_loss: {round(g_loss, 4)}\"\r\n",
        "        )\r\n",
        "\r\n",
        "        rows, cols = 5, 5\r\n",
        "        gen_imgs = GENERATOR.predict(noise) # generating image\r\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\r\n",
        "        fig, axs = plt.subplots(rows, cols)\r\n",
        "        cnt = 0\r\n",
        "        for i in range(rows):\r\n",
        "            for j in range(cols):\r\n",
        "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap=\"gray\")\r\n",
        "                axs[i, j].axis(\"off\")\r\n",
        "                cnt +=1\r\n",
        "        img_name = f\"{epoch}.png\"\r\n",
        "        fig.savefig(os.path.join(PATH, img_name))\r\n",
        "        plt.close()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 - D_loss: 0.5937 D_acc: 0.6953 G_loss: 0.9309\n",
            "1000 - D_loss: 0.5399 D_acc: 0.8125 G_loss: 1.0303\n",
            "2000 - D_loss: 0.6389 D_acc: 0.6094 G_loss: 0.9617\n",
            "3000 - D_loss: 0.5385 D_acc: 0.7344 G_loss: 1.0405\n",
            "4000 - D_loss: 0.6456 D_acc: 0.6719 G_loss: 1.1038\n",
            "5000 - D_loss: 0.7483 D_acc: 0.4844 G_loss: 0.9292\n",
            "6000 - D_loss: 0.5971 D_acc: 0.6562 G_loss: 1.0329\n",
            "7000 - D_loss: 0.6641 D_acc: 0.5938 G_loss: 1.0392\n",
            "8000 - D_loss: 0.6272 D_acc: 0.7188 G_loss: 1.0679\n",
            "9000 - D_loss: 0.6895 D_acc: 0.5156 G_loss: 1.0182\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}